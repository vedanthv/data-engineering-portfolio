{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e3bd6f9-4163-423e-bbd0-d9a49b619f52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_data_source\", \"\")\n",
    "v_data_source = dbutils.widgets.get(\"p_data_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5119843-99e4-46bf-ab3d-8d0f52c56414",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_file_date\", \"2021-03-28\")\n",
    "v_file_date = dbutils.widgets.get(\"p_file_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33b139ec-555b-4ac9-ad28-9b50d26526cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/databricks-course/Formula 1/ingestion-incremental/includes/configurations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dffd7e55-7e2e-4bcb-8828-f2eb08adedae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/databricks-course/Formula 1/ingestion-incremental/includes/common_functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0082552-e479-4319-9697-c7c136335803",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a25ec10-22dd-4bcf-86ae-d7de9f804f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_schema = StructType(fields=[StructField(\"resultId\", IntegerType(), False),\n",
    "                                    StructField(\"raceId\", IntegerType(), True),\n",
    "                                    StructField(\"driverId\", IntegerType(), True),\n",
    "                                    StructField(\"constructorId\", IntegerType(), True),\n",
    "                                    StructField(\"number\", IntegerType(), True),\n",
    "                                    StructField(\"grid\", IntegerType(), True),\n",
    "                                    StructField(\"position\", IntegerType(), True),\n",
    "                                    StructField(\"positionText\", StringType(), True),\n",
    "                                    StructField(\"positionOrder\", IntegerType(), True),\n",
    "                                    StructField(\"points\", FloatType(), True),\n",
    "                                    StructField(\"laps\", IntegerType(), True),\n",
    "                                    StructField(\"time\", StringType(), True),\n",
    "                                    StructField(\"milliseconds\", IntegerType(), True),\n",
    "                                    StructField(\"fastestLap\", IntegerType(), True),\n",
    "                                    StructField(\"rank\", IntegerType(), True),\n",
    "                                    StructField(\"fastestLapTime\", StringType(), True),\n",
    "                                    StructField(\"fastestLapSpeed\", FloatType(), True),\n",
    "                                    StructField(\"statusId\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "736bb042-d0be-491c-a26a-e86d5d93bbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = spark.read \\\n",
    ".schema(results_schema) \\\n",
    ".json(f\"{raw_folder_path}/{v_file_date}/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051d98a9-b752-4d3c-99d6-b3531c23c21a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7205bb6-e2f5-4c87-813d-6b82e21344d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_columns_df = results_df.withColumnRenamed(\"resultId\", \"result_id\") \\\n",
    "                                    .withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    "                                    .withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    "                                    .withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n",
    "                                    .withColumnRenamed(\"positionText\", \"position_text\") \\\n",
    "                                    .withColumnRenamed(\"positionOrder\", \"position_order\") \\\n",
    "                                    .withColumnRenamed(\"fastestLap\", \"fastest_lap\") \\\n",
    "                                    .withColumnRenamed(\"fastestLapTime\", \"fastest_lap_time\") \\\n",
    "                                    .withColumnRenamed(\"fastestLapSpeed\", \"fastest_lap_speed\") \\\n",
    "                                    .withColumn(\"data_source\", lit(v_data_source)) \\\n",
    "                                    .withColumn(\"file_date\", lit(v_file_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d43481c7-a17d-46ad-9be2-67a581d148ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ca849c-9c2a-44b1-bb1d-d6a0116a54bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9649a50a-ca88-4fed-82f5-fa6342e742b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_final_df = results_with_ingestion_date_df.drop(col(\"statusId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa2d55a5-126d-48ee-a2bb-824549c80b3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Incremental Loading - 2 Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb073753-bdcd-4406-999e-fdab0106e8e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make a list of all the distinct race ids using the collect function\n",
    "# now if the table f1_processed.results exists, then...\n",
    "# we alter the table to drop the partition if the race id is already in the parition list.\n",
    "# this way, we can ingest this partition again in the next code block\n",
    "for race_id_list in results_final_df.select(\"race_id\").distinct().collect():\n",
    "    if (spark._jsparkSession.catalog().tableExists(\"f1_processed.results\")):\n",
    "        spark.sql(f\"ALTER TABLE f1_processed.results DROP IF EXISTS PARTITION (race_id = {race_id_list.race_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3a530a-637a-4784-afd1-e3f40c7e3c25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_final_df.write.mode(\"append\").partitionBy('race_id').format(\"parquet\").saveAsTable(\"f1_processed.results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80256e1a-081f-4fe9-a94b-2aa04b366da4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Incremental Load Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f96e5e-2990-4714-81d7-f8671d6e1f59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df = rearrange_partition_column(result_df,'race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8efda758-43e3-4747-a421-c96eec0bb070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "overwrite_partition(results_final_df,'f2_processed','results','race_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8766adb7-de56-4fde-844c-557137c8f49c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This method is better than the first one because spark identiifies which partitions already exist during insertion of the data itself instead of looping over a list everytime and checking which partition is repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c810fad1-834c-4905-b504-77b3a680d20d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-results",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
